{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58edc4e7-9400-4d1c-b418-34f921f81a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fb64c9a-17e6-4655-93fe-1672b84a007b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.18.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.0.7)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from selenium) (0.24.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from selenium) (2023.11.17)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from selenium) (4.9.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from trio~=0.17->selenium) (23.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\hp\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a760a3b9-9a51-468f-b60a-633cd4d6b9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "options = Options()\n",
    "options.headless = False\n",
    "options.add_argument('window-size=1920x1080')\n",
    "\n",
    "chromedriver_path = './chromedriver.exe'\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "target_url = \"https://twitter.com/i/flow/login\"\n",
    "driver.get(target_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1101730d-6b53-4f20-9463-e39f58f901b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "#driver.get(\"https://twitter.com/search-advanced?lang=en\")\n",
    "driver.get(\"https://twitter.com/search?q=from%3A%40POTUS&src=typed_query&f=live\")\n",
    "# login there\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75aae98-4036-43aa-956e-c79612329dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = driver.find_elements(By.XPATH, \"//article[@data-testid='tweet']\")\n",
    "\n",
    "for article in articles[:2]:\n",
    "    tweet_element = article.find_element(By.XPATH, \".//div[@data-testid='tweetText']\")\n",
    "    \n",
    "    tweet_text = tweet_element.text.strip()\n",
    "    print(tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6a5cd7-96f6-4ae6-ae68-9fd4b5a49e06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "links = []\n",
    "wait = WebDriverWait(driver, 10)\n",
    "hrefs = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, '[data-testid=User-Name] a[role=link][href*=status]')))\n",
    "for href in hrefs:\n",
    "    link = href.get_attribute('href')\n",
    "    links.append(link)\n",
    "    print(link)\n",
    "\n",
    "articles = driver.find_elements(By.XPATH, \"//article[@data-testid='tweet']\")\n",
    "\n",
    "for article in articles:\n",
    "    tweet_element = article.find_element(By.XPATH, \".//div[@data-testid='tweetText']\")\n",
    "    \n",
    "    tweet_text = tweet_element.text.strip()\n",
    "    print(tweet_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f12716be-ed1a-4098-8a20-4d2fde5b1714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://twitter.com/lensassaman/status/86117832475148289\n",
      " Aren't they? I really should get back into blogging; haven't since vox.com shut down. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/86117460817887232\n",
      " Now we get to see the classic \"tables turned\" story of the playboy-whose-heart-is-broken and woman-who-gave-up-riches for love. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/86117208601792512\n",
      " so they settled for \"leaving less than a week before the wedding, coincidentally on the date of her first \"single\" release.) \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/86117077500440577\n",
      " (Though literally leaving him at the altar would have made things difficult for the special \"runaway bride\" issue of Playboy, \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/86116921627521024\n",
      " Well, yes — that particular fantasy does have to end. The drama, though, doesn't, hence the \"leaving at the altar\". \n",
      "\n",
      "\n",
      "https://twitter.com/BradStone/status/86108042910896129\n",
      " sweet, i'm reconnecting with people on Google+ that I haven't connected with since I reconnected with them on Facebook! \n",
      "\n",
      "\n",
      "https://twitter.com/emptywheel/status/86102472401223681\n",
      " Shorter Obama: I can't dictate on marriage equality, but I can on war. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/86108442082816001\n",
      " I'd love to know if any of those product namedroppings were paid... \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/86108222997536768\n",
      " it was months ago; I learned of PvZ after Hef tweeted about Crystal playing it in bed on the iPhone. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/86107730359758849\n",
      " he's trapping into archetypal fantasies as deep as \"commoner marries Prince\". \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/86107451958636544\n",
      " oh, but that's part of it! Men dream of \"still having it\" at that age, and women, being The One for the confirmed bachelor. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/86106175615139840\n",
      " Live music and good weather in the Oude Markt. Excellent. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/86105694314569728\n",
      " yep. Though I doubt you'll hear him admit this was planned; no one likes to be had, and he knows that (irony!) \n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_tweet_urls) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m9000\u001b[39m:  \u001b[38;5;66;03m# Continue collecting tweets until we have 1000 unique tweets within the date range\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;66;03m# Sleep for a while to wait for tweets to load\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m         sleep(\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;66;03m#articles = driver.find_elements(By.XPATH, \"//article[@data-testid='tweet']\")\u001b[39;00m\n\u001b[0;32m     26\u001b[0m         articles \u001b[38;5;241m=\u001b[39m wait\u001b[38;5;241m.\u001b[39muntil(EC\u001b[38;5;241m.\u001b[39mvisibility_of_all_elements_located((By\u001b[38;5;241m.\u001b[39mXPATH, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m//article[@data-testid=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "\n",
    "output_directory = 'C:/Users/hp/Documents/scraper/'\n",
    "\n",
    "# Set to store unique tweet URLs\n",
    "unique_tweet_urls = set()\n",
    "\n",
    "# Save tweets to a .txt file\n",
    "output_file_path = output_directory + 'replies_tweets_unprocessed.txt'\n",
    "\n",
    "# Maximum number of retries after encountering a stale element exception\n",
    "max_retries = 100\n",
    "retry_count = 0\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "while len(unique_tweet_urls) < 9000:  # Continue collecting tweets until we have 1000 unique tweets within the date range\n",
    "    try:\n",
    "        # Sleep for a while to wait for tweets to load\n",
    "        sleep(10)\n",
    "        #articles = driver.find_elements(By.XPATH, \"//article[@data-testid='tweet']\")\n",
    "        articles = wait.until(EC.visibility_of_all_elements_located((By.XPATH, \"//article[@data-testid='tweet']\")))\n",
    "        \n",
    "        # Reset retry count\n",
    "        retry_count = 0\n",
    "\n",
    "        # Iterate over each tweet article\n",
    "        for article in articles:\n",
    "            try:\n",
    "                # Find the tweet text element within the article\n",
    "                tweet_element = article.find_element(By.XPATH, \".//div[@data-testid='tweetText']\")\n",
    "                tweet_text = tweet_element.text.strip()\n",
    "\n",
    "                # Remove terms starting with '@' and '#'\n",
    "                tweet_text = ' '.join(word for word in tweet_text.split() if not word.startswith('@') and not word.startswith('#'))\n",
    "\n",
    "                # Extract the tweet date\n",
    "                tweet_date_element = article.find_element(By.XPATH, \".//time\")\n",
    "                tweet_date_str = tweet_date_element.get_attribute(\"datetime\")\n",
    "                tweet_date = datetime.strptime(tweet_date_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "\n",
    "                # Find the tweet URL element within the article\n",
    "                href_element = article.find_element(By.CSS_SELECTOR, '[data-testid=User-Name] a[role=link][href*=status]')\n",
    "                tweet_url = href_element.get_attribute('href')\n",
    "\n",
    "                # Check if the tweet URL is not already saved\n",
    "                if tweet_url not in unique_tweet_urls:\n",
    "                    # Add the tweet URL to the set of unique URLs\n",
    "                    unique_tweet_urls.add(tweet_url)\n",
    "                    # Save the tweet to the file\n",
    "                    with open(output_file_path, 'a', encoding='utf-8') as file:\n",
    "                        print(f\"{tweet_url}\\n {tweet_text} \\n\\n\")\n",
    "                        file.write(f\"{tweet_url}\\n {tweet_text} \\n\\n\")  # Write the tweet text to the file\n",
    "            except Exception as e:\n",
    "                print(\"Error occurred while processing tweet:\", e)\n",
    "\n",
    "        # Scroll to the bottom of the page\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        sleep(10)\n",
    "    except StaleElementReferenceException:\n",
    "        print(\"Stale element reference exception occurred. Retrying...\")\n",
    "        retry_count += 1\n",
    "        if retry_count >= max_retries:\n",
    "            print(\"Maximum retries exceeded. Exiting.\")\n",
    "            break\n",
    "\n",
    "print(f\"Tweets saved to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf70ec99-c31b-4293-b16a-43c0e41b955a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://twitter.com/lensassaman/status/29450954929\n",
      " RT : Photos from the San Francisco Riot as they come in - http://goo.gl/1azu9 | (Finally, traditional media coverage.) \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29461663007\n",
      " Listening to electric bassoons while I work. | http://bit.ly/aIjrMn \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29452892108\n",
      " \"He's a nuisance, that's what he amounts to.\" Oh noes, tattoos and a plastic samari sword! \"A WM with a scruffy beard.\" \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29447668526\n",
      " Officer down? \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29505675163\n",
      " I love that this is happening. Have to write a blog entry on this later: USB dead drops in New York City Walls http://bit.ly/97g31d \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29449344764\n",
      " Police car windows being broken now - 24th and mission. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29449908301\n",
      " needs automatic transcription and a twitter gateway. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29256045654\n",
      " Aliens in my Pants \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29449061411\n",
      " \"I'll just make a list\" (cop, regarding unanswerable calls.) \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29452007208\n",
      " Sounds like it's over. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29452183906\n",
      " (Lobbing vials of putrescine in a riot would be an interesting act of mayhem. I strongly discourage this.) \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29450736252\n",
      " \"Streets are going to be closed for at least a couple more hours.\" \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29445794068\n",
      " Why does not have a webcam out its window? \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29447835758\n",
      " ... yet they have time for jurisdictional quibbles. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29446725237\n",
      " \"Be advised they're throwing *full* bottles of beer at us.\" \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29449009366\n",
      " Moving the crows up 3rd St. now. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29255911082\n",
      " The Young Victoria in my Pants \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29504916065\n",
      " After it's and then probably Rome for the hell of it. Who's going to be at SC? \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29452698467\n",
      " Collection of riot photos: http://bit.ly/br7G97 \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29450562208\n",
      " http://bit.ly/9Jgdk1 | Eternal Sunshine of the Spotless Mind comes a few steps closer to reality. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29409891135\n",
      " Hmm. As I've started watching Doctor Who from the beginning, I'm thinking I really should blog reviews. Hartnell's Doctor is growing on me. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29269328495\n",
      " We Didn't Start The Fire In My Bra (Okay, this is getting absurd.) \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29449314459\n",
      " At least the riot that happens after Whitman wins tomorrow will be more or less subdued. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29450788197\n",
      " \"Demobilize San Mateo\" \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29450050001\n",
      " Pushing the crowd northbound from brannan and 3rd; no, that's the direction of all star donuts! \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29445002574\n",
      " Blimey. Looks like my fridge iced itself shut while I was on holiday. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29445309912\n",
      " Jesus, I'm gone for less than a week, and you riot. San Francisco, I love you. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29451588721\n",
      " For all the excitement the folks in SF are having, it sounds tamer than an average New Year's Eve in Berlin. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29451552441\n",
      " There was no Halloween celebration in the Castro this year, right? Could be that SF needs its yearly Samhain \"ritual of rebellion\". \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29451465551\n",
      " http://bit.ly/a7SG4j is proving incredibly useful. I wish I'd seen it two hours ago, when I looked all this up myself. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29451343695\n",
      " It's hard to tell whether this is more or less out of control than Halloween in the Castro usually is, from here. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29447817251\n",
      " Ugh. The cops are starting to sound... scared now. That's never a good thing. This ain't Oakland; the popo has more guns. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29451426802\n",
      " (Though I have to say, if the bit about the group of people taking pickaxes to the transamerica pyramid was true, that's pretty nuts.) \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29450843131\n",
      " I don't know; an A's and Giants riot could be pretty scary. Maybe the earthquake was preferable. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29269353277\n",
      " pix plz! \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29451834985\n",
      " .@aestetix Max Gluckman's lecture \"Rituals of Rebellion\" is a good place to start, if you're interested in the sociology take on this. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29449665552\n",
      " It's clearly of great strategic importance! Glad I haven't heard All Star Donuts over the scanner yet. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29448038511\n",
      " Worse; it's Greens! \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29448064120\n",
      " When did you move to SF, fool? I missed you! \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29448009779\n",
      " That's good to hear; word over the scanner is it's getting pretty violent. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29452063816\n",
      " Oh, I sympathize. After spending ~6 hours in a putrescine-infused house a few years ago, I had to burn my clothes. [...] \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29452115067\n",
      " [...] but not before I threw them in the wash. I had to get rid of *everything* I washed with them. Putrescine is pretty cool. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/29451978789\n",
      " I took a look at a map; the \"eye of the hurricane\" analogy seems apt. Police activity was in a 200-ish degree arc around DNA. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/27286053337\n",
      " Who's going to BaGG tonight? \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/27420162658\n",
      " Reading about Kai Tak airport. That approach is insane! http://bit.ly/9bgCqU \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/28972395176\n",
      " Home, at last. Yay for uneventful travel. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/27585658895\n",
      " Is anyone else noticing Twitter's API handing out the wrong names for users? \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/27321529295\n",
      " OH: \"The bums were well-dressed, and so were their dogs.\" \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/27600465266\n",
      " Perspectives on Europe: http://bigthink.com/ideas/24357 \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/27762559034\n",
      " DHL Express in the USA is incompetent. I'm beginning to despair of ever receiving my replacement passport. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/27582119192\n",
      " Botox for migraines: http://www.cnn.com/2010/HEALTH/10/15/migraines.botox/index.html \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/27643690823\n",
      " Does anyone have AVIs of Doctor Who from '63 through '74 that you'd be willing to share? I'm having trouble finding them online... \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/28004189059\n",
      " Time to get your whooping cough vaccine booster: http://bit.ly/aYQRVb \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/27575485234\n",
      " DHL is a lot shittier in the US than it is in Europe. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/27643965180\n",
      " Also looking for the 5th through 7th Doctor episodes, and the spin-offs other than Torchwood. Someone has to have them, bigger geeks than I! \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/27297615701\n",
      " Is it me, or is twitter sucking more, and sucking more often, lately? \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/27765539661\n",
      " There's a reason why Michael Arrington isn't a WSJ reporter. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/27420367607\n",
      " Ugh, that has to be one of the worst smells in the world. What died? \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/27260190566\n",
      " The Nikon \"prosumer\" and above bodies are great; a used D50 or D200 beats a D90 any day. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/27645003698\n",
      " Dear followers I'm not following back: do I know you? If so, please tell me! Otherwise, who are you? Why am I interesting? \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/27632475902\n",
      " I haven't had to cast a blackball on anyone in infosec in almost 10 years, but one of my wife's clients is about to change that. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/28058064974\n",
      " Breaking Bad is still one of the best shows on TV today. Watching the third season, finally. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/27248851463\n",
      " The Lower Merion School District webcam spying case has been settled. Disappointed no criminal charges were brought. http://bit.ly/9FescB \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/27644658684\n",
      " Who else in the SFBA is a fountain pen geek? Planning a trip to Castle in the Air sometime soon — want to join me? \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/27589583267\n",
      " It seems some public servants in California need to be reminded that they don't make the laws, and can't arbitrarily arrest people. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/27589339179\n",
      " . tells me that the avocado coevolved with North American giant sloths. That's awesome. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/27951932114\n",
      " If the third season of Torchwood has taught me anything, it's that the British are even more cynical about their government than I realised. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/27951992700\n",
      " . Yep. I've spoken to them every business day, Friday on. I'm never using DHL again if I can help it. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/27647474093\n",
      " Thanks for the pointer to thebox.bz! \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/27290378409\n",
      " We'll miss you! \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/27290364181\n",
      " That's... pitiful. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/28416011860\n",
      " \"Good luck doing 250 comic conventions a year from 2017 on.\" on Martin Freeman's new role as Bilbo. | http://bit.ly/bF97PR \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/27585171815\n",
      " Yep, I was pretty surprised by that myself. \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/27585215578\n",
      " Sure, I'd be happy to. Any particular denominations? \n",
      "\n",
      "\n",
      "https://twitter.com/lensassaman/status/27260262935\n",
      " That trait seems to be shared by an alarming number of people in infosec... \n",
      "\n",
      "\n",
      "Tweets saved to: C:/Users/hp/Documents/scraper/tweets_from_advanced_search.txt\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "\n",
    "output_directory = 'C:/Users/hp/Documents/scraper/'\n",
    "\n",
    "# Find all tweet articles\n",
    "articles = driver.find_elements(By.XPATH, \"//article[@data-testid='tweet']\")\n",
    "\n",
    "# Define the start and end dates\n",
    "start_date = datetime(2007, 7, 12)\n",
    "end_date = datetime(2007, 6, 30)\n",
    "\n",
    "# Set to store unique tweet URLs\n",
    "unique_tweet_urls = set()\n",
    "\n",
    "# Save tweets to a .txt file\n",
    "output_file_path = output_directory + 'tweets_from_advanced_search.txt'\n",
    "\n",
    "while len(unique_tweet_urls) < 60:  # Continue collecting tweets until we have 1000 unique tweets within the date range\n",
    "      # Sleep for a while to wait for tweets to load\n",
    "    sleep(10)\n",
    "    articles = driver.find_elements(By.XPATH, \"//article[@data-testid='tweet']\")\n",
    "\n",
    "    # Iterate over each tweet article\n",
    "    for article in articles:\n",
    "        try:\n",
    "            # Find the tweet text element within the article\n",
    "            tweet_element = article.find_element(By.XPATH, \".//div[@data-testid='tweetText']\")\n",
    "            tweet_text = tweet_element.text.strip()\n",
    "\n",
    "            # Remove terms starting with '@' and '#'\n",
    "            tweet_text = ' '.join(word for word in tweet_text.split() if not word.startswith('@') and not word.startswith('#'))\n",
    "\n",
    "            # Extract the tweet date\n",
    "            tweet_date_element = article.find_element(By.XPATH, \".//time\")\n",
    "            tweet_date_str = tweet_date_element.get_attribute(\"datetime\")\n",
    "            tweet_date = datetime.strptime(tweet_date_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "\n",
    "            # Find the tweet URL element within the article\n",
    "            href_element = article.find_element(By.CSS_SELECTOR, '[data-testid=User-Name] a[role=link][href*=status]')\n",
    "            tweet_url = href_element.get_attribute('href')\n",
    "\n",
    "            # Check if the tweet date falls within the specified range\n",
    "            \n",
    "                # Check if the tweet URL is not already saved\n",
    "            if tweet_url not in unique_tweet_urls:\n",
    "                  # Add the tweet URL to the set of unique URLs\n",
    "                 unique_tweet_urls.add(tweet_url)\n",
    "                    # Save the tweet to the file\n",
    "                 with open(output_file_path, 'a', encoding='utf-8') as file:\n",
    "                     print(f\"{tweet_url}\\n {tweet_text} \\n\\n\")\n",
    "                     file.write(f\"{tweet_url}\\n {tweet_text} \\n\\n\")  # Write the tweet text to the file\n",
    "        except Exception as e:\n",
    "            print(\"Error occurred while processing tweet:\", e)\n",
    "\n",
    "    # Scroll to the bottom of the page\n",
    "    try:\n",
    "        # Scroll to the bottom of the page\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        sleep(10)\n",
    "    except StaleElementReferenceException:\n",
    "        print(\"Stale element reference exception occurred. Stopping scraping.\")\n",
    "        break\n",
    "    \n",
    "    \n",
    "print(f\"Tweets saved to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ad3678-0e18-4faa-b0f6-2c96a90d3a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "\n",
    "output_directory = 'C:/Users/hp/Documents/scraper/'\n",
    "\n",
    "# Find all tweet articles\n",
    "articles = driver.find_elements(By.XPATH, \"//article[@data-testid='tweet']\")\n",
    "\n",
    "# Define the start and end dates\n",
    "start_date = datetime(2007, 7, 12)\n",
    "end_date = datetime(2007, 6, 30)\n",
    "\n",
    "tweets = set()  # Use a set to store unique tweets with their dates\n",
    "\n",
    "# Save tweets to a .txt file\n",
    "output_file_path = output_directory + 'tweets_from_advanced_search.txt'\n",
    "\n",
    "while len(tweets) < 6000:  # Continue collecting tweets until we have 1000 unique tweets within the date range\n",
    "    articles = driver.find_elements(By.XPATH, \"//article[@data-testid='tweet']\")\n",
    "    \n",
    "    # Iterate over each tweet article\n",
    "    for article in articles:\n",
    "        try:\n",
    "            # Find the tweet text element within the article\n",
    "            tweet_element = article.find_element(By.XPATH, \".//div[@data-testid='tweetText']\")\n",
    "            tweet_text = tweet_element.text.strip()\n",
    "\n",
    "            # Remove text associated with @ mentions\n",
    "            tweet_text = ' '.join(word for word in tweet_text.split() if not word.startswith('@'))\n",
    "\n",
    "            # Extract the tweet date\n",
    "            tweet_date_element = article.find_element(By.XPATH, \".//time\")\n",
    "            tweet_date_str = tweet_date_element.get_attribute(\"datetime\")\n",
    "            tweet_date = datetime.strptime(tweet_date_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "\n",
    "            # Find the tweet URL element within the article\n",
    "            href_element = article.find_element(By.CSS_SELECTOR, '[data-testid=User-Name] a[role=link][href*=status]')\n",
    "            tweet_url = href_element.get_attribute('href')\n",
    "\n",
    "            # Check if the tweet date falls within the specified range\n",
    "            \n",
    "            tweets.add((tweet_text, tweet_date))  # Add the tweet to the set of unique tweets with its date\n",
    "            print(f\"{tweet_text} \\n\")\n",
    "            with open(output_file_path, 'a', encoding='utf-8') as file:\n",
    "                file.write(f\"{tweet_url}\\n {tweet_text} \\n\\n\")  # Write the tweet text to the file\n",
    "        except Exception as e:\n",
    "            print(\"Error occurred while processing tweet:\", e)\n",
    "\n",
    "    # Scroll to the bottom of the page\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    sleep(5)\n",
    "    print(\"Scrolled to the bottom of the page\")\n",
    "\n",
    "print(f\"Tweets saved to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562bf05d-9483-428d-a1ad-c3a6d47c39f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#This query is working best with tweeturls and tweet texts\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "\n",
    "\n",
    "output_directory = 'C:/Users/hp/Documents/scraper/'\n",
    "\n",
    "# Find all tweet articles\n",
    "articles = driver.find_elements(By.XPATH, \"//article[@data-testid='tweet']\")\n",
    "\n",
    "# Define the start and end dates\n",
    "start_date = datetime(2011, 4, 12)\n",
    "end_date = datetime(2011, 6, 30)\n",
    "\n",
    "tweets = set()  # Use a set to store unique tweets with their dates\n",
    "\n",
    "# Save tweets to a .txt file\n",
    "output_file_path = output_directory + 'tweets_with_links_commulative2.txt'\n",
    "\n",
    "\n",
    "while len(tweets) < 6000:  # Continue collecting tweets until we have 1000 unique tweets within the date range\n",
    "    articles = driver.find_elements(By.XPATH, \"//article[@data-testid='tweet']\")\n",
    "    \n",
    "    # Iterate over each tweet article\n",
    "    for article in articles:\n",
    "        try:\n",
    "            # Find the tweet text element within the article\n",
    "            tweet_element = article.find_element(By.XPATH, \".//div[@data-testid='tweetText']\")\n",
    "            tweet_text = tweet_element.text.strip()\n",
    "\n",
    "            # Remove text associated with @ mentions\n",
    "            tweet_text = ' '.join(word for word in tweet_text.split() if not word.startswith('@'))\n",
    "\n",
    "            # Extract the tweet date\n",
    "            tweet_date_element = article.find_element(By.XPATH, \".//time\")\n",
    "            tweet_date_str = tweet_date_element.get_attribute(\"datetime\")\n",
    "            tweet_date = datetime.strptime(tweet_date_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "            \n",
    "            \n",
    "            # Find the tweet URL element within the article\n",
    "            href_element = article.find_element(By.CSS_SELECTOR, '[data-testid=User-Name] a[role=link][href*=status]')\n",
    "            tweet_url = href_element.get_attribute('href')\n",
    "\n",
    "            # Check if the tweet date falls within the specified range\n",
    "            if start_date <= tweet_date <= end_date:\n",
    "                tweets.add((tweet_text, tweet_date))  # Add the tweet to the set of unique tweets with its date\n",
    "                print(f\"{tweet_text} \\n\")\n",
    "                with open(output_file_path, 'a', encoding='utf-8') as file:\n",
    "                    file.write(f\"{tweet_url}\\n {tweet_text} \\n\\n\")  # Write the tweet text to the file\n",
    "        except Exception as e:\n",
    "            print(\"Error occurred while processing tweet:\", e)\n",
    "\n",
    "            \n",
    "            \n",
    "    driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "    sleep(30)\n",
    "\n",
    "print(f\"Tweets saved to: {output_file_path}\")   \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89afb19f-334f-4131-993a-254447ee0f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#This is working best for .txt format file\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Specific directory to save the text file\n",
    "output_directory = 'C:/Users/hp/Documents/scraper/'\n",
    "\n",
    "# Define the start and end dates\n",
    "start_date = datetime(2011, 4, 12)\n",
    "end_date = datetime(2011, 6, 30)\n",
    "\n",
    "tweets = set()  # Use a set to store unique tweets with their dates\n",
    "\n",
    "# Save tweets to a .txt file\n",
    "output_file_path = output_directory + 'tweets_text.txt'\n",
    "\n",
    "while len(tweets) < 9000:  # Continue collecting tweets until we have 1000 unique tweets within the date range\n",
    "    articles = driver.find_elements(By.XPATH, \"//article[@data-testid='tweet']\")\n",
    "    \n",
    "    # Check if the articles are not visible\n",
    "    if not articles:\n",
    "        print(\"No tweets found. Waiting for 5 minutes before retrying...\")\n",
    "        break\n",
    "        #sleep(300)  # Wait for 5 minutes before retrying\n",
    "        #continue\n",
    "    \n",
    "    for article in articles:\n",
    "        try:\n",
    "            tweet_element = article.find_element(By.XPATH, \".//div[@data-testid='tweetText']\")\n",
    "            tweet_text = tweet_element.text.strip()\n",
    "\n",
    "            # Remove text associated with @ mentions\n",
    "            tweet_text = ' '.join(word for word in tweet_text.split() if not word.startswith('@'))\n",
    "\n",
    "            # Extract the tweet date\n",
    "            tweet_date_element = article.find_element(By.XPATH, \".//time\")\n",
    "            tweet_date_str = tweet_date_element.get_attribute(\"datetime\")\n",
    "            tweet_date = datetime.strptime(tweet_date_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "\n",
    "            # Check if the tweet date falls within the specified range\n",
    "            if start_date <= tweet_date <= end_date:\n",
    "                tweets.add((tweet_text, tweet_date))  # Add the tweet to the set of unique tweets with its date\n",
    "                print(f\"{tweet_text} \\n\")\n",
    "                with open(output_file_path, 'a', encoding='utf-8') as file:\n",
    "                    file.write(f\"{tweet_text} \\n\\n\")  # Write the tweet text to the file\n",
    "        except Exception as e:\n",
    "            print(\"Error occurred while processing tweet:\", e)\n",
    "\n",
    "    driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "    sleep(5)\n",
    "\n",
    "print(f\"Tweets saved to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c9667d67-9996-44e2-bdc0-de0c4b013b15",
   "metadata": {},
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Find all tweet articles\n",
    "articles = driver.find_elements(By.XPATH, \"//article[@data-testid='tweet']\")\n",
    "\n",
    "# Iterate over each tweet article\n",
    "for article in articles:\n",
    "    # Find the tweet text element within the article\n",
    "    tweet_element = article.find_element(By.XPATH, \".//div[@data-testid='tweetText']\")\n",
    "    \n",
    "    # Find the tweet URL element within the article\n",
    "    href_element = article.find_element(By.CSS_SELECTOR, '[data-testid=User-Name] a[role=link][href*=status]')\n",
    "    \n",
    "    # Extract the tweet text\n",
    "    tweet_text = tweet_element.text.strip()\n",
    "    \n",
    "    # Extract the tweet URL\n",
    "    tweet_url = href_element.get_attribute('href')\n",
    "    \n",
    "    # Print the tweet text and URL\n",
    "    print(\"Tweet Text:\", tweet_text)\n",
    "    print(\"Tweet URL:\", tweet_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be438ebb-cf90-47c3-a8a8-b530d83ce768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Specific directory to save the text file\n",
    "output_directory = 'C:/Users/hp/Documents/scraper/'\n",
    "\n",
    "# Define the start and end dates\n",
    "start_date = datetime(2011, 4, 12)\n",
    "end_date = datetime(2011, 6, 30)\n",
    "\n",
    "tweets = set()  # Use a set to store unique tweets with their dates\n",
    "\n",
    "# Save tweets to a .txt file\n",
    "output_file_path = output_directory + 'tweets_text.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "    while len(tweets) < 9000:  # Continue collecting tweets until we have 1000 unique tweets within the date range\n",
    "        articles = driver.find_elements(By.XPATH, \"//article[@data-testid='tweet']\")\n",
    "        for article in articles:\n",
    "            try:\n",
    "                tweet_element = article.find_element(By.XPATH, \".//div[@data-testid='tweetText']\")\n",
    "                tweet_text = tweet_element.text.strip()\n",
    "\n",
    "                # Remove text associated with @ mentions\n",
    "                tweet_text = ' '.join(word for word in tweet_text.split() if not word.startswith('@'))\n",
    "\n",
    "                # Extract the tweet date\n",
    "                tweet_date_element = article.find_element(By.XPATH, \".//time\")\n",
    "                tweet_date_str = tweet_date_element.get_attribute(\"datetime\")\n",
    "                tweet_date = datetime.strptime(tweet_date_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "\n",
    "                # Check if the tweet date falls within the specified range\n",
    "                if start_date <= tweet_date <= end_date:\n",
    "                    tweets.add((tweet_text, tweet_date))  # Add the tweet to the set of unique tweets with its date\n",
    "                    print(f\"{tweet_text} - {tweet_date}\")\n",
    "                    file.write(f\"{tweet_text} \\n\\n\")  # Write the tweet text to the file\n",
    "            except Exception as e:\n",
    "                print(\"Error occurred while processing tweet:\", e)\n",
    "\n",
    "        driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "        sleep(5)\n",
    "\n",
    "print(f\"Tweets saved to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17572455-2cc8-4fa1-be08-4ecb3e32f5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Specific directory to save the text file\n",
    "output_directory = 'C:/Users/hp/Documents/scraper/'\n",
    "\n",
    "# Define the start and end dates\n",
    "start_date = datetime(2011, 4, 12)\n",
    "end_date = datetime(2011, 6, 30)\n",
    "\n",
    "tweets = set()  # Use a set to store unique tweets with their dates\n",
    "\n",
    "while len(tweets) < 9000:  # Continue collecting tweets until we have 1000 unique tweets within the date range\n",
    "    articles = driver.find_elements(By.XPATH, \"//article[@data-testid='tweet']\")\n",
    "    for article in articles:\n",
    "        try:\n",
    "            tweet_element = article.find_element(By.XPATH, \".//div[@data-testid='tweetText']\")\n",
    "            tweet_text = tweet_element.text.strip()\n",
    "\n",
    "            # Remove text associated with @ mentions\n",
    "            tweet_text = ' '.join(word for word in tweet_text.split() if not word.startswith('@'))\n",
    "\n",
    "            # Extract the tweet date\n",
    "            tweet_date_element = article.find_element(By.XPATH, \".//time\")\n",
    "            tweet_date_str = tweet_date_element.get_attribute(\"datetime\")\n",
    "            tweet_date = datetime.strptime(tweet_date_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "\n",
    "            # Check if the tweet date falls within the specified range\n",
    "            if start_date <= tweet_date <= end_date:\n",
    "                tweets.add((tweet_text, tweet_date))  # Add the tweet to the set of unique tweets with its date\n",
    "                print(f\"{tweet_text} - {tweet_date}\")\n",
    "        except Exception as e:\n",
    "            print(\"Error occurred while processing tweet:\", e)\n",
    "\n",
    "    driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "    sleep(5)\n",
    "\n",
    "# Sort tweets by date\n",
    "sorted_tweets = sorted(tweets, key=lambda x: x[1])\n",
    "\n",
    "# Save tweets to a .txt file\n",
    "output_file_path = output_directory + 'tweets_text.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "    for tweet_text, tweet_date in sorted_tweets:\n",
    "        file.write(f\"{tweet_text} \\n\\n\")\n",
    "\n",
    "print(f\"Tweets saved to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbd8d31-8202-4ac6-b51d-0dfd6206fc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from time import sleep\n",
    "\n",
    "# Specific directory to save the text file\n",
    "output_directory = 'C:/Users/hp/Documents/scraper/'\n",
    "\n",
    "# Define the start and end dates\n",
    "start_date = datetime(2011, 4, 12)\n",
    "end_date = datetime(2011, 6, 30)\n",
    "\n",
    "articles = driver.find_elements(By.XPATH, \"//*[@id='id__mjtrgd4uhm']/span[2]\")\n",
    "\n",
    "while len(tweets) < 10:  # Continue collecting tweets until we have 100 tweets\n",
    "    for article in articles:\n",
    "        tweet_element = article.find_element(By.XPATH, \".//div[@data-testid='tweetText']\")\n",
    "        tweet_text = tweet_element.text\n",
    "        \n",
    "        # Remove text associated with @ mentions\n",
    "        at_index = tweet_text.find('@')\n",
    "        while at_index != -1:  # Loop until all mentions are removed\n",
    "            space_index = tweet_text.find(' ', at_index)\n",
    "            newline_index = tweet_text.find('\\n', at_index)\n",
    "            if space_index != -1 and (newline_index == -1 or space_index < newline_index):\n",
    "                tweet_text = tweet_text[:at_index] + tweet_text[space_index:]\n",
    "            elif newline_index != -1:\n",
    "                tweet_text = tweet_text[:at_index] + tweet_text[newline_index:]\n",
    "            else:\n",
    "                tweet_text = tweet_text[:at_index]\n",
    "            at_index = tweet_text.find('@')\n",
    "        \n",
    "        # Extract the tweet date\n",
    "        tweet_date_element = article.find_element(By.XPATH, \".//time\")\n",
    "        tweet_date_str = tweet_date_element.get_attribute(\"datetime\")\n",
    "        tweet_date = datetime.strptime(tweet_date_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "        \n",
    "        # Check if the tweet date falls within the specified range\n",
    "        if start_date <= tweet_date <= end_date:\n",
    "            # Save tweet to the file\n",
    "            output_file_path = output_directory + 'tweets1.txt'\n",
    "            with open(output_file_path, 'a', encoding='utf-8') as file:\n",
    "                file.write(f\"{tweet_text} \\n\\n\")\n",
    "                print(f\"{tweet_text}\")\n",
    "        \n",
    "            # Increment the tweet count\n",
    "            tweets.append((tweet_text, tweet_date))\n",
    "        \n",
    "    driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "    sleep(5)\n",
    "    articles = driver.find_elements(By.XPATH, \"//article[@data-testid='tweet']\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed71da0d-422a-4290-a2b3-3994d11df855",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Specific directory to save the text file\n",
    "output_directory = 'C:/Users/hp/Documents/scraper/'\n",
    "\n",
    "# Define the start and end dates\n",
    "start_date = datetime(2011, 4, 12)\n",
    "end_date = datetime(2011, 6, 30)\n",
    "\n",
    "# Counter to keep track of the number of tweets collected\n",
    "tweet_count = 0\n",
    "\n",
    "while tweet_count < 100:  # Continue collecting tweets until 100 tweets are collected\n",
    "    articles = driver.find_elements(By.XPATH, \"//article[@data-testid='tweet']\")\n",
    "    for article in articles:\n",
    "        try:\n",
    "            tweet_element = article.find_element(By.XPATH, \".//div[@data-testid='tweetText']\")\n",
    "            tweet_text = tweet_element.text.strip()\n",
    "\n",
    "            # Remove text associated with @ mentions\n",
    "            tweet_text = ' '.join(word for word in tweet_text.split() if not word.startswith('@'))\n",
    "\n",
    "            # Extract the tweet date\n",
    "            tweet_date_element = article.find_element(By.XPATH, \".//time\")\n",
    "            tweet_date_str = tweet_date_element.get_attribute(\"datetime\")\n",
    "            tweet_date = datetime.strptime(tweet_date_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "\n",
    "            # Check if the tweet date falls within the specified range\n",
    "            if start_date <= tweet_date <= end_date:\n",
    "                # Save tweet to the file\n",
    "                output_file_path = output_directory + 'tweets.txt'\n",
    "                with open(output_file_path, 'a', encoding='utf-8') as file:\n",
    "                    file.write(f\"{tweet_text} - {tweet_date}\\n\")\n",
    "                    print(f\"{tweet_text} - {tweet_date}\")\n",
    "                tweet_count += 1\n",
    "        except Exception as e:\n",
    "            print(\"Error occurred while processing tweet:\", e)\n",
    "\n",
    "    driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "    sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345467d3-8721-44d3-86e5-6836800cfb5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5300467-8b2f-42a1-87db-71482af1896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = driver.find_elements(By.XPATH, \"//article[@data-testid='tweet']\")\n",
    "print(\"Length of articles: \", len(articles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99c12f5-8dcc-4a0a-aebb-565b8bae962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = driver.find_elements(By.XPATH, \"//article[@data-testid='tweet']\")\n",
    "print(\"Length of articles: \", len(articles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f5c953-e166-4c90-9971-f6fa68374614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Specific directory to save the text file\n",
    "output_directory = 'C:/Users/hp/Documents/scraper/'\n",
    "\n",
    "# Define the start date (July 2007) and end date (June 2011)\n",
    "start_date = datetime(2007, 7, 1)\n",
    "end_date = datetime(2011, 6, 30)\n",
    "\n",
    "# Open files for writing tweets and retweets\n",
    "tweets_output_file_path = output_directory + 'tweets.txt'\n",
    "retweets_output_file_path = output_directory + 'retweets.txt'\n",
    "\n",
    "with open(tweets_output_file_path, 'a', encoding='utf-8') as tweets_file, \\\n",
    "     open(retweets_output_file_path, 'a', encoding='utf-8') as retweets_file:\n",
    "\n",
    "    articles = driver.find_elements(By.XPATH, \"//article[@data-testid='tweet']\")\n",
    "    for article in articles:\n",
    "        # Extract the tweet text\n",
    "        tweet_element = article.find_element(By.XPATH, \".//div[@data-testid='tweetText']\")\n",
    "        tweet_text = tweet_element.text\n",
    "        \n",
    "        # Extract the retweet text (if any)\n",
    "        retweet_element = article.find_element(By.XPATH, \".//div[@data-testid='retweet']\")\n",
    "        retweet_text = retweet_element.text if retweet_element else None\n",
    "        \n",
    "        # Extract the tweet date\n",
    "        tweet_date_element = article.find_element(By.XPATH, \".//time\")\n",
    "        tweet_date_str = tweet_date_element.get_attribute(\"datetime\")\n",
    "        tweet_date = datetime.strptime(tweet_date_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "        \n",
    "        # Check if the tweet date falls within the specified time range\n",
    "        if start_date <= tweet_date <= end_date:\n",
    "            if tweet_text:\n",
    "                # Preprocess tweet text to remove mentions and hashtags\n",
    "                tweet_text = ' '.join(word for word in tweet_text.split() if not word.startswith('@') and not word.startswith('#'))\n",
    "                tweets_file.write(tweet_text + '\\n')\n",
    "            if retweet_text:\n",
    "                # Preprocess retweet text to remove mentions and hashtags\n",
    "                retweet_text = ' '.join(word for word in retweet_text.split() if not word.startswith('@') and not word.startswith('#'))\n",
    "                retweets_file.write(retweet_text + '\\n')\n",
    "            \n",
    "            print(f\"{tweet_text} - {tweet_date}\")\n",
    "            print(f\"Retweet: {retweet_text}\")\n",
    "        else:\n",
    "            break\n",
    "\n",
    "print(f\"Tweets saved to: {tweets_output_file_path}\")\n",
    "print(f\"Retweets saved to: {retweets_output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0c9a8d-7f57-4628-9ab0-5017e1f6dae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(zip(Tweets),columns=['Tweets'])\n",
    "df.head()\n",
    "\n",
    "df.to_excel(r\"C:\\Users\\hp\\Documents\\scraper\\tweets_live.xlsx\",index=False)\n",
    "\n",
    "import os\n",
    "os.system('start \"excel\" \"C:\\Users\\hp\\Documents\\scraper\\\\tweets_live.xlsx\"')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
